{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28f1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  \n",
    "\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178146c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a concise, helpful assistant.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Just say 'hello.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e203ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "resp = client.chat.complete(\n",
    "    model=\"mistral-large-latest\",  # or \"mistral-small-latest\" / \"open-mistral-7b\"\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"You are concise and factual.\"},\n",
    "        {\"role\":\"user\",\"content\":\"What is ReAct prompting in agents?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f00d0149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ReAct (Reasoning and Acting)** is a prompting technique for **LLM-based agents** that interleaves **reasoning traces** (thought processes) with **actions** (e.g., tool/API calls) in a structured, iterative loop. Developed by *Yao et al. (2022)*, it aims to improve transparency, interpretability, and task-solving capability by:\n",
      "\n",
      "1. **Reasoning**: The agent generates internal thoughts (e.g., *\"I need to find X to answer Y\"*).\n",
      "2. **Acting**: The agent takes concrete steps (e.g., searching a database, calling an API).\n",
      "3. **Observation**: The agent receives feedback (e.g., API response, error message).\n",
      "4. **Repeat**: The cycle continues until the task is resolved.\n",
      "\n",
      "---\n",
      "### **Key Features**\n",
      "- **Interleaved Structure**:\n",
      "  ```plaintext\n",
      "  Thought: [reasoning step]\n",
      "  Action: [tool/API call, e.g., `search[query]`]\n",
      "  Observation: [result]\n",
      "  ```\n",
      "- **Dynamic Adaptation**: Adjusts actions based on observations (e.g., retries after failures).\n",
      "- **Tool Integration**: Works with external tools (e.g., calculators, web search).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1aad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from mistralai import Mistral\n",
    "\n",
    "with client.chat.stream(\n",
    "    model=\"mistral-small-latest\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Give a 3-bullet summary of LangChain.\"}],\n",
    "    temperature=0.3,\n",
    ") as stream:\n",
    "    full = []\n",
    "    for event in stream:\n",
    "        # token chunks arrive as 'ChatCompletionChunk' objects\n",
    "        if hasattr(event, \"choices\"):\n",
    "            for choice in event.choices:\n",
    "                delta = choice.delta\n",
    "                if delta and delta.content:\n",
    "                    sys.stdout.write(delta.content)\n",
    "                    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f04c891e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 dims\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "texts = [\n",
    "    \"RAG uses retrieval to ground generation.\",\n",
    "    \"AutoGPT chains tool-using steps for goals.\"\n",
    "]\n",
    "emb = client.embeddings.create(model=\"mistral-embed\", inputs=texts)\n",
    "print(len(emb.data[0].embedding), \"dims\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb7b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Dehradun is clear with a temperature of **26Â°C**.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current weather for a city.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"city\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "def get_weather(city: str):\n",
    "    return {\"city\": city, \"temp_c\": 26, \"status\": \"clear\"}\n",
    "\n",
    "tool_impl = {\"get_weather\": get_weather}\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather in Dehradun?\"}\n",
    "]\n",
    "# 1) FIRST MODEL CALL\n",
    "\n",
    "resp = client.chat.complete(\n",
    "    model=\"mistral-small-latest\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "# check if tool call exists\n",
    "choice = resp.choices[0]\n",
    "tool_calls = choice.message.tool_calls\n",
    "\n",
    "if tool_calls:\n",
    "    call = tool_calls[0]            \n",
    "    name = call.function.name\n",
    "    args = json.loads(call.function.arguments)\n",
    "\n",
    "    result = tool_impl[name](**args)\n",
    "\n",
    "    # add tool call + result back into conversation\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"id\": call.id,\n",
    "                \"function\": {\n",
    "                    \"name\": name,\n",
    "                    \"arguments\": call.function.arguments\n",
    "                },\n",
    "                \"type\": \"function\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps(result),\n",
    "        \"tool_call_id\": call.id\n",
    "    })\n",
    "\n",
    "    # 2) SECOND MODEL CALL\n",
    "\n",
    "    final = client.chat.complete(\n",
    "        model=\"mistral-small-latest\",\n",
    "        messages=messages\n",
    "    )\n",
    "    print(final.choices[0].message.content)\n",
    "else:\n",
    "    print(choice.message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23d691b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sam Altman is a prominent American entrepreneur, investor, and programmer. He is best known as the former president of Y Combinator, a highly influential startup accelerator. Altman co-founded Loopt, a location-based social networking service, and has been involved in various other ventures and investments in the technology and startup ecosystems. He is also known for his work in artificial intelligence and his involvement with OpenAI.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, time\n",
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"now\",\n",
    "            \"description\": \"Return current unix time.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def now():\n",
    "    return {\"ts\": time.time()}\n",
    "\n",
    "IMPL = {\"now\": now}\n",
    "\n",
    "def run():\n",
    "    msgs = [{\"role\": \"system\", \"content\": \"Answer briefly. Use tools if needed.\"}]\n",
    "\n",
    "    while True:\n",
    "        user = input(\"> \").strip()\n",
    "        if not user or user.lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "\n",
    "        msgs.append({\"role\": \"user\", \"content\": user})\n",
    "\n",
    "        # First call: see if model wants to use a tool\n",
    "        resp = client.chat.complete(\n",
    "            model=\"mistral-small-latest\",\n",
    "            messages=msgs,\n",
    "            tools=TOOLS,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        choice = resp.choices[0]\n",
    "        \n",
    "        tool_calls = choice.message.tool_calls\n",
    "        final_text = None\n",
    "\n",
    "        if tool_calls:\n",
    "            # Model requested a tool\n",
    "            call = tool_calls[0]\n",
    "            name = call.function.name\n",
    "            args = json.loads(call.function.arguments)\n",
    "\n",
    "            result = IMPL[name](**args)\n",
    "\n",
    "            # Add assistant tool call message\n",
    "            msgs.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": call.id,\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": name,\n",
    "                            \"arguments\": call.function.arguments\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "\n",
    "            # Add the tool's result\n",
    "\n",
    "            msgs.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": call.id,\n",
    "                \"content\": \"When you need to use a tool, respond ONLY in JSON: {\\\"tool\\\": \\\"name\\\", \\\"args\\\": {...}}. Otherwise, answer normally.\"\n",
    "            })\n",
    "\n",
    "            # Second call: model produces final answer\n",
    "            final = client.chat.complete(\n",
    "                model=\"mistral-small-latest\",\n",
    "                messages=msgs\n",
    "            )\n",
    "            final_text = final.choices[0].message.content\n",
    "        else:\n",
    "            # No tool, answer directly\n",
    "            final_text = choice.message.content\n",
    "\n",
    "        print(final_text)\n",
    "        msgs.append({\"role\": \"assistant\", \"content\": final_text})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4057fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
